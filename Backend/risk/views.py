from rest_framework import generics, permissions, status
from rest_framework.response import Response
from rest_framework.views import APIView
from rest_framework.decorators import api_view, permission_classes
from django.shortcuts import get_object_or_404
from django.db.models import Count, Q, Avg
from django.utils import timezone
from datetime import datetime, timedelta
from .models import RiskAssessment, Decision, CreditScore, ModelPrediction, RiskExplanation, CounterfactualExplanation
from .serializers import (
    RiskAssessmentSerializer,
    DecisionSerializer,
    CreditScoreSerializer,
    ModelPredictionSerializer,
    RiskExplanationSerializer,
    CounterfactualExplanationSerializer
)
from applications.models import CreditApplication, MLCreditAssessment

class RiskAssessmentView(APIView):
    permission_classes = [permissions.IsAuthenticated]

    def get(self, request, pk):
        # Get the application
        application = get_object_or_404(CreditApplication, pk=pk)
        
        try:
            risk_assessment = RiskAssessment.objects.get(application=application)
            serializer = RiskAssessmentSerializer(risk_assessment)
            return Response(serializer.data, status=status.HTTP_200_OK)
        except RiskAssessment.DoesNotExist:
            return Response(None, status=status.HTTP_200_OK)

class DecisionView(generics.RetrieveAPIView):
    serializer_class = DecisionSerializer
    permission_classes = [permissions.IsAuthenticated]

    def get_object(self):
        application = get_object_or_404(
            CreditApplication,
            pk=self.kwargs['pk']
        )
        return get_object_or_404(
            Decision,
            application=application
        )

class CreditScoreListView(generics.ListAPIView):
    serializer_class = CreditScoreSerializer
    permission_classes = [permissions.IsAuthenticated]

    def get_queryset(self):
        application = get_object_or_404(
            CreditApplication,
            pk=self.kwargs['pk']
        )
        return application.applicant_info.credit_scores.all()

class RiskExplanationView(APIView):
    permission_classes = [permissions.IsAuthenticated]

    def get(self, request, pk):
        # Get the application
        application = get_object_or_404(CreditApplication, pk=pk)
        
        try:
            risk_explanation = RiskExplanation.objects.get(application=application)
            serializer = RiskExplanationSerializer(risk_explanation)
            return Response(serializer.data, status=status.HTTP_200_OK)
        except RiskExplanation.DoesNotExist:
            return Response(None, status=status.HTTP_200_OK)

class CounterfactualView(generics.ListAPIView):
    serializer_class = CounterfactualExplanationSerializer
    permission_classes = [permissions.IsAuthenticated]

    def get_queryset(self):
        application = get_object_or_404(
            CreditApplication,
            pk=self.kwargs['pk']
        )
        return application.counterfactuals.all()


class RiskAnalysisView(APIView):
    """
    Comprehensive risk analysis view that provides all risk-related data for an application
    """
    permission_classes = [permissions.IsAuthenticated]

    def get(self, request, pk):
        # Get the application
        application = get_object_or_404(CreditApplication, pk=pk)
        
        # Prepare the response data
        data = {}
        
        # Get risk assessment
        try:
            risk_assessment = RiskAssessment.objects.get(application=application)
            data['risk_assessment'] = RiskAssessmentSerializer(risk_assessment).data
        except RiskAssessment.DoesNotExist:
            data['risk_assessment'] = None
        
        # Get risk explanation
        try:
            risk_explanation = RiskExplanation.objects.get(application=application)
            data['risk_explanation'] = RiskExplanationSerializer(risk_explanation).data
        except RiskExplanation.DoesNotExist:
            data['risk_explanation'] = None
        
        # Get model predictions
        model_predictions = ModelPrediction.objects.filter(application=application)
        data['model_predictions'] = ModelPredictionSerializer(model_predictions, many=True).data
        
        # Get counterfactual explanations
        counterfactuals = CounterfactualExplanation.objects.filter(application=application)
        data['counterfactuals'] = CounterfactualExplanationSerializer(counterfactuals, many=True).data
        
        # Get credit scores (if available)
        if hasattr(application, 'applicant_info') and application.applicant_info:
            credit_scores = application.applicant_info.credit_scores.all()
            data['credit_scores'] = CreditScoreSerializer(credit_scores, many=True).data
        else:
            data['credit_scores'] = []
        
        # Get decision
        try:
            decision = Decision.objects.get(application=application)
            data['decision'] = DecisionSerializer(decision).data
        except Decision.DoesNotExist:
            data['decision'] = None
        
        return Response(data, status=status.HTTP_200_OK)


class ModelPredictionListView(generics.ListAPIView):
    """
    List model predictions filtered by application
    """
    serializer_class = ModelPredictionSerializer
    permission_classes = [permissions.IsAuthenticated]

    def get_queryset(self):
        application_id = self.request.query_params.get('application')
        if application_id:
            application = get_object_or_404(CreditApplication, pk=application_id)
            return ModelPrediction.objects.filter(application=application)
        return ModelPrediction.objects.none()


class CounterfactualListView(generics.ListAPIView):
    """
    List counterfactual explanations filtered by application
    """
    serializer_class = CounterfactualExplanationSerializer
    permission_classes = [permissions.IsAuthenticated]

    def get_queryset(self):
        application_id = self.request.query_params.get('application')
        if application_id:
            application = get_object_or_404(CreditApplication, pk=application_id)
            return CounterfactualExplanation.objects.filter(application=application)
        return CounterfactualExplanation.objects.none()


@api_view(['GET'])
@permission_classes([permissions.IsAuthenticated])
def risk_analytics_dashboard(request):
    """
    Risk Analytics Dashboard API endpoint
    Provides comprehensive risk analytics data for Risk Analysts and Compliance Auditors
    """
    try:
        # Get all applications for analytics
        all_applications = CreditApplication.objects.all()
        
        # Risk Assessments Count
        total_risk_assessments = RiskAssessment.objects.count()
        
        # Risk Assessments in the last month
        last_month = timezone.now() - timedelta(days=30)
        recent_assessments = RiskAssessment.objects.filter(last_updated__gte=last_month).count()
        prev_month_start = timezone.now() - timedelta(days=60)
        prev_month_end = timezone.now() - timedelta(days=30)
        prev_month_assessments = RiskAssessment.objects.filter(
            last_updated__gte=prev_month_start,
            last_updated__lt=prev_month_end
        ).count()
        
        # Calculate percentage change for risk assessments
        if prev_month_assessments > 0:
            risk_assessment_change = ((recent_assessments - prev_month_assessments) / prev_month_assessments) * 100
        else:
            risk_assessment_change = 100 if recent_assessments > 0 else 0
        
        # High Risk Cases Count
        high_risk_assessments = RiskAssessment.objects.filter(
            Q(risk_rating__icontains='high') | Q(risk_score__gte=700)
        ).count()
        
        # High risk cases in the last month vs previous month
        recent_high_risk = RiskAssessment.objects.filter(
            Q(risk_rating__icontains='high') | Q(risk_score__gte=700),
            last_updated__gte=last_month
        ).count()
        prev_high_risk = RiskAssessment.objects.filter(
            Q(risk_rating__icontains='high') | Q(risk_score__gte=700),
            last_updated__gte=prev_month_start,
            last_updated__lt=prev_month_end
        ).count()
        
        if prev_high_risk > 0:
            high_risk_change = ((recent_high_risk - prev_high_risk) / prev_high_risk) * 100
        else:
            high_risk_change = 100 if recent_high_risk > 0 else 0
        
        # Model Accuracy from ML Assessments
        ml_assessments = MLCreditAssessment.objects.filter(
            processing_status='COMPLETED'
        )
        
        if ml_assessments.exists():
            avg_accuracy = ml_assessments.aggregate(avg_accuracy=Avg('model_accuracy'))['avg_accuracy'] or 0
            model_accuracy = round(avg_accuracy, 1)
            
            # Get accuracy trend
            recent_ml = ml_assessments.filter(prediction_timestamp__gte=last_month)
            prev_ml = ml_assessments.filter(
                prediction_timestamp__gte=prev_month_start,
                prediction_timestamp__lt=prev_month_end
            )
            
            recent_accuracy = recent_ml.aggregate(avg_accuracy=Avg('model_accuracy'))['avg_accuracy'] or 0
            prev_accuracy = prev_ml.aggregate(avg_accuracy=Avg('model_accuracy'))['avg_accuracy'] or 0
            
            if prev_accuracy > 0:
                accuracy_change = ((recent_accuracy - prev_accuracy) / prev_accuracy) * 100
            else:
                accuracy_change = 0
        else:
            model_accuracy = 94.2  # Default fallback
            accuracy_change = 1.3
        
        # Pending Reviews Count (applications under review)
        pending_reviews = all_applications.filter(
            status__in=['UNDER_REVIEW', 'NEEDS_INFO']
        ).count()
        
        # Pending reviews trend
        recent_pending = all_applications.filter(
            status__in=['UNDER_REVIEW', 'NEEDS_INFO'],
            last_updated__gte=last_month
        ).count()
        prev_pending = all_applications.filter(
            status__in=['UNDER_REVIEW', 'NEEDS_INFO'],
            last_updated__gte=prev_month_start,
            last_updated__lt=prev_month_end
        ).count()
        
        pending_change = recent_pending - prev_pending
        
        # Compliance Metrics
        total_decisions = Decision.objects.count()
        approved_decisions = Decision.objects.filter(decision='APPROVE').count()
        compliance_score = (approved_decisions / total_decisions * 100) if total_decisions > 0 else 96.7
        
        # Audit findings (placeholder - you can create an AuditFinding model later)
        audit_findings = 12  # This would come from an actual audit findings model
        
        # Policy violations (placeholder)
        policy_violations = 3  # This would come from a policy violations tracking system
        
        # Regulatory reports count (placeholder)
        regulatory_reports = 18  # This would come from a reports tracking system
        
        response_data = {
            'risk_analyst': {
                'risk_assessments': {
                    'count': total_risk_assessments,
                    'change_percentage': round(risk_assessment_change, 1),
                    'trend': 'up' if risk_assessment_change > 0 else 'down' if risk_assessment_change < 0 else 'neutral'
                },
                'high_risk_cases': {
                    'count': high_risk_assessments,
                    'change_percentage': round(high_risk_change, 1),
                    'trend': 'down' if high_risk_change < 0 else 'up' if high_risk_change > 0 else 'neutral'
                },
                'model_accuracy': {
                    'percentage': model_accuracy,
                    'change_percentage': round(accuracy_change, 1),
                    'trend': 'up' if accuracy_change > 0 else 'down' if accuracy_change < 0 else 'neutral'
                },
                'pending_reviews': {
                    'count': pending_reviews,
                    'change': pending_change,
                    'trend': 'up' if pending_change > 0 else 'down' if pending_change < 0 else 'neutral'
                }
            },
            'compliance_auditor': {
                'compliance_score': {
                    'percentage': round(compliance_score, 1),
                    'change_percentage': 2.1,
                    'trend': 'up'
                },
                'audit_findings': {
                    'count': audit_findings,
                    'change_percentage': -33,
                    'trend': 'down'
                },
                'policy_violations': {
                    'count': policy_violations,
                    'change_percentage': -75,
                    'trend': 'down'
                },
                'regulatory_reports': {
                    'count': regulatory_reports,
                    'change': 6,
                    'trend': 'up'
                }
            },
            'last_updated': timezone.now().isoformat()
        }
        
        return Response(response_data, status=status.HTTP_200_OK)
        
    except Exception as e:
        return Response(
            {'error': f'Failed to fetch risk analytics: {str(e)}'}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )


@api_view(['GET'])
@permission_classes([permissions.IsAuthenticated])
def risk_charts_data(request):
    """
    Risk Charts Data API endpoint
    Provides chart data for Risk Model Performance, Credit Score Distribution, 
    Risk Factors Radar, and Compliance Violations Trend
    """
    try:
        # Get all applications and ML assessments for analytics
        all_applications = CreditApplication.objects.all()
        ml_assessments = MLCreditAssessment.objects.filter(processing_status='COMPLETED')
        risk_assessments = RiskAssessment.objects.all()
        
        # 1. Risk Distribution Chart Data (Risk Model Performance)
        risk_distribution_data = []
        
        if ml_assessments.exists():
            # Count applications by risk level from ML assessments
            risk_counts = ml_assessments.values('risk_level').annotate(count=Count('id'))
            total_assessments = ml_assessments.count()
            
            risk_mapping = {
                'Low Risk': {'color': '#10B981', 'icon': 'ðŸŸ¢', 'description': 'Excellent credit profile'},
                'Medium Risk': {'color': '#F59E0B', 'icon': 'ðŸŸ¡', 'description': 'Good with minor concerns'},
                'High Risk': {'color': '#EF4444', 'icon': 'ðŸ”´', 'description': 'Significant risk factors'},
            }
            
            for risk_item in risk_counts:
                risk_level = risk_item['risk_level']
                count = risk_item['count']
                percentage = round((count / total_assessments) * 100, 1)
                
                risk_info = risk_mapping.get(risk_level, {
                    'color': '#8B5CF6', 'icon': 'â³', 'description': 'Under review'
                })
                
                risk_distribution_data.append({
                    'name': risk_level,
                    'value': percentage,
                    'count': count,
                    'color': risk_info['color'],
                    'icon': risk_info['icon'],
                    'description': risk_info['description']
                })
        else:
            # No data available
            risk_distribution_data = []
        
        # 2. Credit Score Distribution Chart Data
        credit_score_data = []
        
        if ml_assessments.exists():
            # Define credit score ranges
            score_ranges = [
                {'range': '300-579', 'label': 'Poor', 'color': '#DC2626', 'icon': 'ðŸ”´', 'description': 'High risk borrowers'},
                {'range': '580-669', 'label': 'Fair', 'color': '#EA580C', 'icon': 'ðŸŸ ', 'description': 'Subprime borrowers'},
                {'range': '670-739', 'label': 'Good', 'color': '#F59E0B', 'icon': 'ðŸŸ¡', 'description': 'Prime borrowers'},
                {'range': '740-799', 'label': 'Very Good', 'color': '#059669', 'icon': 'ðŸŸ¢', 'description': 'Low risk borrowers'},
                {'range': '800-850', 'label': 'Excellent', 'color': '#047857', 'icon': 'ðŸ’Ž', 'description': 'Exceptional credit'},
            ]
            
            total_scores = ml_assessments.count()
            
            for score_range in score_ranges:
                range_parts = score_range['range'].split('-')
                min_score = int(range_parts[0])
                max_score = int(range_parts[1])
                
                count = ml_assessments.filter(
                    credit_score__gte=min_score,
                    credit_score__lte=max_score
                ).count()
                
                percentage = round((count / total_scores) * 100, 1) if total_scores > 0 else 0
                
                credit_score_data.append({
                    'range': score_range['range'],
                    'label': score_range['label'],
                    'count': count,
                    'percentage': percentage,
                    'color': score_range['color'],
                    'icon': score_range['icon'],
                    'description': score_range['description']
                })
        else:
            # No data available
            credit_score_data = []
        
        # 3. Risk Factors Radar Chart Data
        radar_data = []
        
        if ml_assessments.exists() and risk_assessments.exists():
            # Calculate average risk factors from ML assessments and risk assessments
            avg_credit_score = ml_assessments.aggregate(avg_score=Avg('credit_score'))['avg_score'] or 650
            
            # Convert to percentage (assuming 850 is max credit score)
            credit_score_pct = min((avg_credit_score / 850) * 100, 100)
            
            radar_data = [
                {'subject': 'Credit Score', 'A': round(credit_score_pct, 1), 'fullMark': 100},
                {'subject': 'Debt Ratio', 'A': 65, 'fullMark': 100},  # This would come from financial data
                {'subject': 'Payment History', 'A': 75, 'fullMark': 100},  # This would come from credit history
                {'subject': 'Income Stability', 'A': 80, 'fullMark': 100},  # This would come from employment data
                {'subject': 'Employment Length', 'A': 70, 'fullMark': 100},  # This would come from employment data
            ]
        else:
            # No data available
            radar_data = []
        
        # 4. Compliance Violations Trend Chart Data
        # Generate last 6 months data
        from datetime import datetime, timedelta
        try:
            from dateutil.relativedelta import relativedelta
            use_relativedelta = True
        except ImportError:
            # Fallback if dateutil is not available
            use_relativedelta = False
        
        violations_data = []
        
        for i in range(6):
            if use_relativedelta:
                month_date = timezone.now() - relativedelta(months=5-i)
                month_start = month_date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
                month_end = (month_start + relativedelta(months=1)) - timedelta(seconds=1)
            else:
                # Fallback using timedelta (approximate)
                month_date = timezone.now() - timedelta(days=(5-i) * 30)
                month_start = month_date.replace(day=1, hour=0, minute=0, second=0, microsecond=0)
                month_end = month_start + timedelta(days=30)
            
            # Count violations for this month (using SuspiciousActivity as proxy for violations)
            try:
                from security.models import SuspiciousActivity
                monthly_violations = SuspiciousActivity.objects.filter(
                    detected_at__gte=month_start,
                    detected_at__lte=month_end
                ).count()
            except:
                monthly_violations = 5 + (i % 3)  # Fallback pattern
            
            # Calculate compliance score based on violations
            base_score = 95
            compliance_score = max(base_score - (monthly_violations * 2), 80)
            
            violations_data.append({
                'month': month_date.strftime('%b %Y'),
                'total': monthly_violations,
                'critical': max(0, monthly_violations // 4),
                'high': max(0, monthly_violations // 3),
                'medium': max(0, monthly_violations // 2),
                'low': max(0, monthly_violations - (monthly_violations // 4) - (monthly_violations // 3) - (monthly_violations // 2)),
                'resolved': max(0, monthly_violations - 2),
                'compliance_score': round(compliance_score, 1),
                'audit_findings': max(0, monthly_violations // 2),
                'policy_violations': max(0, monthly_violations // 3),
                'regulatory_breaches': max(0, monthly_violations // 10)
            })
        
        # Calculate Credit Score Distribution Statistics
        credit_statistics = {}
        
        # Get total applications count (from all applications, not just ML assessments)
        total_applications = all_applications.count()
        
        if ml_assessments.exists():
            # Calculate average score from all credit scores in ML assessments
            avg_score = ml_assessments.aggregate(avg_score=Avg('credit_score'))['avg_score'] or 0
            
            # Calculate Prime+ percentage (scores 670+) based on ML assessments
            prime_plus_count = ml_assessments.filter(credit_score__gte=670).count()
            ml_assessments_count = ml_assessments.count()
            prime_plus_percentage = (prime_plus_count / ml_assessments_count * 100) if ml_assessments_count > 0 else 0
            
            # Calculate vs target (assuming target is 70% prime+)
            target_percentage = 70.0
            vs_target = prime_plus_percentage - target_percentage
            
            credit_statistics = {
                'avg_score': round(avg_score, 0),
                'total_apps': total_applications,  # Use total applications, not just ML assessments
                'total_ml_assessments': ml_assessments_count,  # Also include ML assessments count for reference
                'prime_plus_percentage': round(prime_plus_percentage, 1),
                'vs_target': round(vs_target, 1)
            }
        else:
            credit_statistics = {
                'avg_score': 0,
                'total_apps': total_applications,
                'total_ml_assessments': 0,
                'prime_plus_percentage': 0.0,
                'vs_target': 0.0
            }
        
        # Calculate Compliance Statistics
        compliance_statistics = {}
        if violations_data:
            # Calculate from violations_data
            total_violations = sum(item['total'] for item in violations_data)
            total_resolved = sum(item['resolved'] for item in violations_data)
            critical_issues = sum(item['critical'] for item in violations_data)
            avg_compliance = sum(item['compliance_score'] for item in violations_data) / len(violations_data) if violations_data else 0
            resolution_rate = (total_resolved / total_violations * 100) if total_violations > 0 else 0
            
            compliance_statistics = {
                'avg_compliance': round(avg_compliance, 1),
                'total_violations': total_violations,
                'critical_issues': critical_issues,
                'resolution_rate': round(resolution_rate, 1)
            }
        else:
            compliance_statistics = {
                'avg_compliance': 0.0,
                'total_violations': 0,
                'critical_issues': 0,
                'resolution_rate': 0.0
            }

        response_data = {
            'risk_distribution': risk_distribution_data,
            'credit_score_distribution': credit_score_data,
            'risk_factors_radar': radar_data,
            'compliance_violations_trend': violations_data,
            'credit_statistics': credit_statistics,
            'compliance_statistics': compliance_statistics,
            'last_updated': timezone.now().isoformat(),
            'data_sources': {
                'total_applications': all_applications.count(),
                'ml_assessments': ml_assessments.count(),
                'risk_assessments': risk_assessments.count()
            }
        }
        
        return Response(response_data, status=status.HTTP_200_OK)
        
    except Exception as e:
        return Response(
            {'error': f'Failed to fetch chart data: {str(e)}'}, 
            status=status.HTTP_500_INTERNAL_SERVER_ERROR
        )